{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2017, Google published eager execution in TensorFlow, which greatly helped the visualization during training. With eager execution, we are able to print out variables and other models without running a session as in the traditional TensorFlow program. And this would be very benificial for new learners. In this notebook, we will use eager execution to learn the basics of TensorFlow such as the data type, GPU assignment and other APIs such as dataset and automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and activate eager execution.\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.add(12, 37) = tf.Tensor(49, shape=(), dtype=int32)\n",
      "tf.add([1, 3], [4, 8]) = tf.Tensor([ 5 11], shape=(2,), dtype=int32)\n",
      "tf.reduce_sum([-4, 0, 8, 10]) = tf.Tensor(14, shape=(), dtype=int32)\n",
      "\n",
      "x = tf.Tensor([[25 33]], shape=(1, 2), dtype=int32)\n",
      "x.shape: (1, 2)\n",
      "x.dtype: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# Tensors calculation and transformation. Python types are converted automatically to Tensor types.\n",
    "print('tf.add(12, 37) =', tf.add(12, 37))\n",
    "print('tf.add([1, 3], [4, 8]) =', tf.add([1, 3], [4, 8]))\n",
    "print('tf.reduce_sum([-4, 0, 8, 10]) =', tf.reduce_sum([-4, 0, 8, 10]))\n",
    "print()\n",
    "\n",
    "x = tf.matmul([[1, 3]], [[4, 6], [7, 9]])  # tensors have two properties- shape and dtype\n",
    "print('x =', x)\n",
    "print('x.shape:', x.shape)\n",
    "print('x.dtype:', x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy array: [[1. 1. 1.]]\n",
      "tensor: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float64)\n",
      "numpy sum: [[4. 4. 4.]]\n",
      "numpy tensor: [[3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "# Numpy compatibility. TensorFlow tensors and Numpy arrays can be transofrmed easily in type and automatically in calculation.\n",
    "import numpy as np\n",
    "\n",
    "numpy_arr = np.ones((1, 3))\n",
    "print('numpy array:', numpy_arr)\n",
    "tensor = tf.multiply(numpy_arr, 3)\n",
    "print('tensor:', tensor)\n",
    "numpy_sum = np.add(tensor, 1)\n",
    "print('numpy sum:', numpy_sum)\n",
    "numpy_tensor = tensor.numpy()\n",
    "print('numpy tensor:', numpy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is avaliable and if calculation or variable is a certain GPU.\n",
    "print(tf.test.is_gpu_available())\n",
    "print(x.device.endswith('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU matrix multiplication running time: 0.016754460334777833 s\n"
     ]
    }
   ],
   "source": [
    "# CPU or GPU assignment.\n",
    "import time\n",
    "\n",
    "def time_matmul(x):\n",
    "    start = time.time()\n",
    "    for loop in range(10):\n",
    "        tf.matmul(x, x)\n",
    "    return (time.time() - start) / 10\n",
    "\n",
    "with tf.device('CPU:0'):\n",
    "    x = tf.random_uniform([1000, 1000])\n",
    "    assert x.device.endswith('CPU:0')\n",
    "    print('CPU matrix multiplication running time:', time_matmul(x), 's')\n",
    "    \n",
    "if tf.test.is_gpu_available():\n",
    "    with tf.device('GPU:0'):\n",
    "        x = tf.random_uniform([1000, 1000])\n",
    "        assert x.device.endswith('GPU:0')\n",
    "        print('GPU matrix multiplication running time:', time_matmul(x), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Dataset API.\n",
    "tensors = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(tensors)\n",
    "for t in tensors:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_dx = tf.Tensor(\n",
      "[[18. 18. 18.]\n",
      " [18. 18. 18.]\n",
      " [18. 18. 18.]], shape=(3, 3), dtype=float32)\n",
      "dz_dy = tf.Tensor(18.0, shape=(), dtype=float32)\n",
      "\n",
      "dz_dx = tf.Tensor(\n",
      "[[18. 18. 18.]\n",
      " [18. 18. 18.]\n",
      " [18. 18. 18.]], shape=(3, 3), dtype=float32)\n",
      "dz_dy = tf.Tensor(18.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation in gradient tape.\n",
    "x = tf.ones((3, 3))\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "dz_dx = t.gradient(z, x)  # gradient of z wrt. x\n",
    "print('dz_dx =', dz_dx)\n",
    "\n",
    "# Notice GradientTape.gradient can only be called once on non-persistent tapes.\n",
    "x = tf.ones((3, 3))\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "dz_dy = t.gradient(z, y)  # gradient of z wrt. y\n",
    "print('dz_dy =', dz_dy)\n",
    "\n",
    "print()\n",
    "# In order to solve this inconvinience, we can create a persistent gradient tape and delete it after use.\n",
    "x = tf.ones((3, 3))\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "dz_dx = t.gradient(z, x)\n",
    "print('dz_dx =', dz_dx)\n",
    "dz_dy = t.gradient(z, y)\n",
    "print('dz_dy =', dz_dy)\n",
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(80.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# A good thing of gradient tape is that they can handle statements like ;'if', 'while', etc.\n",
    "def f(x, y):\n",
    "    output = 1\n",
    "    for i in range(y):\n",
    "        if x > 1 and x < 5:\n",
    "            output = tf.multiply(output, x)\n",
    "    return output\n",
    "\n",
    "x = tf.convert_to_tensor(2.0)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    z = f(x, 5)\n",
    "print(t.gradient(z, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dx = tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "d2y_dx2 = tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Gradient tape can also deal with high order gradients.\n",
    "x = tf.convert_to_tensor(1.0)  # Note that if eager execution is enabled, 'tf.Variable' is not valid.\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    with tf.GradientTape() as t2:\n",
    "        t2.watch(x)\n",
    "        y = x * x * x\n",
    "    dy_dx = t2.gradient(y, x)\n",
    "    t.watch(x)\n",
    "d2y_dx2 = t.gradient(dy_dx, x)\n",
    "\n",
    "print('dy_dx =', dy_dx)\n",
    "print('d2y_dx2 =', d2y_dx2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
